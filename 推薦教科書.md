#
```
tf.keras 技術者們必讀！深度學習攻略手冊
施威銘研究室 著  旗標科技  2020-02-13
```

```
第 0 章 行前準備：NumPy 陣列快速上手
0-0 陣列的 shape 與軸、階、維度
NumPy陣列的優點及特色
0-1 NumPy 陣列的屬性與型別轉換
0-2 建立 NumPy 陣列 (array)
0-3 陣列切片 (slicing)：截取陣列片段
0-4 陣列的重塑與轉置
陣列重塑(reshape)：改變陣列形狀
矩陣轉置(transpose)：將矩陣的直行與橫列交換

0-5 陣列間對應元素 (element-wise) 的運算
0-6 陣列擴張 (broadcasting)
0-7 組合多個索引來批次設值或取值
0-8 沿著陣列的某一軸做運算

第 1 章 初探 Keras：建構第一隻神經網路
1-0 神經網路基礎
1-1 神經網路的學習方式
單一神經元的學習方式
整個神經網路的學習方式
1-2 Keras 及 tf.keras 速覽
多後端的Keras
Tensorflow的tf.keras


1-3 用 6 行程式建構神經網路
建立空的神經網路模型
加入第一層的神經層(兼具輸入層功能)
加入第二層的神經層(做為輸出層)
指定訓練及評量方式來編譯模型

1-4 訓練神經網路的流程 - 以辨識手寫數字圖片為例
0. 準備訓練所需的資料
1. 資料預處理(Preprocess)
2. 建立與編譯模型
3. 訓練模型
4. 評估模型成效及修正
5. 用模型預測答案
6. 模型的儲存及載入

1-5 實驗：以自製手寫數字圖片測試
動手製作3種手寫數字圖片
實驗1：測試原始圖片, 但不做前置影像調整
實驗2：做前置影像調整後再測試
實驗3~7：測試各種可能影響準確率的因素
實驗3：測試數字位置對準確率的影響
實驗4：測試數字大小對準確率的影響
實驗5：測試線條邊緣漸層對準確率的影響
實驗6：測試線條粗細對準確率的影響
實驗7：測試線條深淺對準確率的影響
實驗8：找出自製圖片的最佳調整方式
實驗9：用最佳調整方式重新訓練模型

1-6 NumPy 陣列的點積運算 (dot product)
向量的點積運算
矩陣的點積運算
多軸陣列的點積運算

1-7 密集神經網路的張量運算

第 2 章 序列模型與密集神經網路
2-0 再論序列模型
在序列模型中加入神經層
模型的屬性
用get_layer()取得模型中的神經層
用pop()刪除序列模型最後面的神經層
建立本章範例使用的工具模組

2-1 存取模型的結構或權重參數
取得與還原模型的結構
儲存與載入模型的結構
取得與還原模型的權重參數
儲存與載入模型的權重參數

2-2 模型的編譯：compile() 
2-2-0 Compile()的loss參數
實驗：加大或縮小損失值對訓練成效的影響
2-2-1 Compile()的metrics參數
2-2-2 Compile()的optimizer參數
2-2-3 compile()全部參數的詳細用法
2-2-4 compile()可能發生的例外

2-3 模型的訓練：fit()
fit()的傳回值
fit()可能發生的例外
實驗：不同batch_size對訓練成效的影響

2-4 逐批生成資料來訓練、驗證模型
2-5 模型的評估：evaluate()
evaluate()的傳回值

2-6 模型的預測：predict()
predict()的傳回值
predict()可能發生的例外

2-7 用一小批資料做訓練、評估、或預測


2-8 用生成器及 Sequence 物件專用的方法來做訓練、評估、或預測

2-9 密集層 (Dense Layer)
建立密集層時的參數設定
密集層的輸入與輸出shape
使用常規化參數來減少過度配適


2-10 實例：二元分類 - 使用 IMDB 做正負評分類
0. 載入IMDB資料集
1. 資料預處理
2. 建立與編譯模型
3. 模型的訓練、驗證、與調整
4. 模型的確認與評估成效
5. 用訓練好的模型預測新資料


2-11 實例：多元分類 - 將路透社新聞分類成 46 個主題
0. 載入路透社資料集
1. 資料預處理
2. 建立與編譯模型
3. 模型的訓練、驗證、與調整
4. 模型的確認與評估成效
5. 用訓練好的模型預測新資料
直接用「類別索引」為標籤做訓練


2-12 實例：迴歸預測 - 使用美國研究所錄取資料集來預測錄取機率
0. 載入美國研究所錄取資料集
1. 資料預處理
2. 建立與編譯模型
3. 模型的訓練、驗證、與調整
4. 模型的確認與評估成效
5. 用訓練好的模型預測新資料
6. 實驗：使用不同的神經層數、神經元數、及批次量做測試

第 3 章 卷積神經網路 (CNN)
3-0 從密集神經網路到卷積神經網路
辨識尺寸更大、資料更豐富的圖片
為什麼權重太多不利於神經網路？
卷積神經網路 (CNN) 的基本架構
使用Keras建構一個簡單的CNN
3-1 卷積層 (Convolutional Layer) 
卷積核(Kernel)
卷積運算與特徵圖(featuremaps)
影像辨識的直觀理解
使用Keras建立卷積層
設定初始器(initializer)
3-2 池化層 (Pooling Layer) 
最大池化MaxPooling2D
平均池化AveragePooling2D
降維方式的抉擇
3-3 展平層 (Flatten)
3-4 密集層 (Dense Layer)
3-5 丟棄層 (Dropout)
3-6 實戰：訓練 CNN 辨識 CIFAR-10 圖片資料集
載入CIFAR-10圖片資料集
對圖片資料進行特徵縮放(Featurescaling)
將數字標籤轉為One-hot編碼
建立卷積神經網路

第 4 章 循環神經網路 (RNN)
4-0 以 SimpleRNN 找到資料中的週期
4-1 時序資料預處理 Sequence Preprocessing
4-2 Stateful RNN
實例：Dense VS RNN VS Stateful RNN 
預測未來溫度
溫度資料預處理
使用密集網路預測未來溫度
使用 RNN 預測未來溫度
使用 stateful RNN 預測未來溫度


4-3 文件資料預處理 Text Preprocessing
文字轉序列
Token 法 
雜湊法
序列對齊
文字轉 one-hot
文字轉 multi-hot
文字轉詞向量
4-4 以嵌入層(Embedding layer)實作 IMDB 分類效果
4-5 使用循環丟棄法避免過度配適問題
4-6 堆疊循環層
4-7 長短期記憶 (LSTM) 和閘控循環單元 (GRU)
4-8 雙向循環層
4-9 結果與討論
4-10 實驗：RNN 和 IMDB 的各種測試
實驗 1：RNN 的煉金術配方
實驗 2：RNN 一定要搭配嵌入層嗎？
序列格式在 LSTM 的測試
one-hot 格式在 LSTM 的測試
LSTM 使用不同格式的比較 
實驗 3：自然語言處理的特徵工程
詞袋(Bag-of-Word) 
N 元語法(N-gram) 

第 5 章 函數式 API
5-0 函數式 API (Functional API) 快速上手
函數式 API 的建模方式
Input() 及 Model() 的參數設定
5-1 多輸入模型
5-1-0 可以合併多個分支的合併層
5-1-1 實例：建立有 3 個輸入層的模型
5-1-2 用程式產生訓練樣本及答案
5-1-3 訓練有 3 個輸入層的模型
5-1-4 利用 EMA 找出訓練成效最好的週期
5-1-5 重新訓練到最佳週期並評估成效
5-1-6 實驗：如果改用 2 種樣本資料來訓練模型呢？
5-2 多輸出模型
5-2-0 實例：將前面範例多加一個「評價」輸出層
5-2-1 產生「評價」標籤資料, 並依評價修改銷量
5-2-2 編譯多輸出模型
5-2-3 自訂評量函式
5-2-4 訓練多輸出模型
5-2-5 找出並訓練到最佳週期, 然後產生測試資料評估成效
5-2-6 實驗：增加樣本數量以提升準確率
5-3 函數式 API 的更多應用
5-3-0 內部分岔的有向無循環模型
5-3-1 層的共用與權重共享
5-3-2 層內的節點 (node)
5-3-3 實例：判斷 2 張手寫數字圖片是否為同一個數字
5-3-4 將模型做為層來使用
5-3-5 實例：在新模型中套用已訓練好的 CNN 模型
5-4 繪製模型的結構圖
5-5 實例：用「故事與問題」訓練雙輸入的 RNN 問答模型
5-5-0 資料集說明：The (20) QA bAbI tasks 資料集
5-5-1 下載 QA 資料集
5-5-1 撰寫解析檔案內容的 get_sqa() 函式
5-5-2 直接讀取壓縮檔的內容並轉換為 QA 資料集
5-5-3 資料預處理
5-5-4 建立及編譯雙輸入模型
5-5-5 訓練模型並評估成效
5-5-6 實驗 0：測試所有適用的 QA 任務
5-5-7 實驗 1：在故事樣本中加入無關 (非支持答案) 的敘述


第 6 章 預先訓練自己的中文詞向量
6-0 為什麼要預先訓練詞向量
6-1 Word2vec 實作原理
CBOW 連續詞袋模型
Skip-gram 跳字模型
進階 Skip-gram 模型
6-2 建立並訓練 Word2vec 神經網路
6-2-0 建立完整的 Word2vec 架構
6-2-1 取得原始資料 (語料)
維基百科語料
社區問答語料
6-2-2 資料預處理的介紹
解析 JSON 形式：json 套件
簡體轉繁體：opencc 套件
斷詞：Jieba 套件
6-2-3 資料預處理：產生訓練 Word2vec 所需的資料集
Wiki 資料集


第 7 章 進階應用
7-0 用 Callback 監控訓練過程
7-0-0 EarlyStopping
7-0-1 ModelCheckpoint
7-0-2 ReduceLROnPlateau
7-0-3 LearningRateScheduler
7-0-4 CSVLogger
7-0-5 撰寫你自己的 Callback
7-1 用 TensorBoard 解析訓練過程的歷史記錄
7-1-0 基本使用方式
7-1-1 tf.TensorBoard Callback 的其他參數
7-2 圖檔及影像處理技巧
7-2-0 檢視本節所附的樣本圖檔：貓狗資料集
7-2-1 可提供批次影像資料的 ImageDataGenerator
7-2-2 實例：用貓狗的照片檔來訓練、驗證、評估、預測模型
7-2-3 ImageDataGenerator 的資料擴增功能
7-2-4 實例：使用資料擴增提升訓練成效
7-2-5 實驗：進一步提升貓狗辨識的準確率
7-2-6 實驗：將資料擴增法及 Dropout 層應用在樣本不足的案例
7-3 各種 CNN 經典模型的建構祕方
7-3-0 LeNet
7-3-1 AlexNet
實驗：梯度消失 (Vanishing gradient)
7-3-2 VGG
7-3-3 Network in Network (NiN)
7-3-4 GoogLeNet：Inception-V1
7-3-5 Inception-V2、V3
實驗：在 40 層以上的神經網路中使用 BN 層
實驗：BN 層讓 Sigmoid 復活了
Inception-V4、Inception-ResNet
7-3-7 Xception
實驗：SeparableConv2D vs Conv2D
7-3-5 ResNet
7-3-6 DenseNet (Densely Connected Convolutional Networks)
7-4 遷移學習 - 以預訓練好的經典模型 VGG16 為例
7-4-0 什麼是遷移學習 (transfer learning)
7-4-1 萃取出資料的特徵
7-4-2 將經典 CNN 移植到新模型之中
7-4-3 模型的微調 (fine-tuning)
```
```
