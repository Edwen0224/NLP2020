#
```

```

```
K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters
Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu ji, Guihong Cao, Daxin Jiang, Ming Zhou
https://arxiv.org/abs/2002.01808

problem of injecting knowledge into large pre-trained models like BERT and RoBERTa
problem of catastrophic forgetting
```
```
AdapterFusion: Non-Destructive Task Composition for Transfer Learning
Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, Iryna Gurevych
https://arxiv.org/abs/2005.00247
```

```
Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov
https://arxiv.org/abs/1901.02860
```

```


```



```


```


```


```



```


```



```


```



```


```



```


```



```


```
