# review
```
A Survey of Deep Learning Techniques for Neural Machine Translation
Shuoheng Yang, Yuxin Wang, Xiaowen Chu
https://arxiv.org/abs/2002.07526
```
```
Pre-trained Models for Natural Language Processing: A Survey
Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, Xuanjing Huang
https://arxiv.org/abs/2003.08271
```
```
Yonatan Belinkov and James Glass. 2019. 
Analysis Methods in Neural Language Processing: A Survey.
Transactions of the Association for Computational Linguistics, 7:49–72.
```
# NNLM(2003)[Neural Network Language Model]
```
Bengio, Y., Ducharme, R., Vincent, P., and Janvin, C. (2003). 
A neural probabilistic language model.
J. Mach. Learn. Res., 3, 1137–1155.
```
```
https://www.itread01.com/content/1545932290.html
https://zhuanlan.zhihu.com/p/21240807
https://medium.com/%E7%A8%8B%E5%BC%8F%E5%B7%A5%E4%BD%9C%E7%B4%A1\
/a-neural-probabilistic-language-model-%E8%AB%96%E6%96%87%E7%AD%86%E8%A8%98-61f4c5cecee7
```
# seq2seq Model(2014)
```
Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio
https://arxiv.org/abs/1406.1078
```
```
Sequence to Sequence Learning with Neural Networks
Ilya Sutskever, Oriol Vinyals, Quoc V. Le
https://arxiv.org/abs/1409.3215
```
```
Multi-task Sequence to Sequence Learning
Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, Lukasz Kaiser
https://arxiv.org/abs/1511.06114
```
```
Convolutional sequence to sequence learning.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin.  
arXiv preprint arXiv:1705.03122v2, 2017.
```

# Attention(2014) 注意力模型 (Attention Model)
```
Neural Machine Translation by Jointly Learning to Align and Translate
Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio
https://arxiv.org/abs/1409.0473
```
```
Show, Attend and Tell: Neural Image Caption Generation with Visual Attention
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio
https://arxiv.org/abs/1502.03044
```
```
Effective Approaches to Attention-based Neural Machine Translation
Minh-Thang Luong, Hieu Pham, Christopher D. Manning
https://arxiv.org/abs/1508.04025

Global attentional model vs local attentional model
```
```
Multiple Object Recognition with Visual Attention
Jimmy Ba, Volodymyr Mnih, Koray Kavukcuoglu
https://arxiv.org/abs/1412.7755
```

### 各式各樣的注意力機制
```
自我注意力 (self attention)
```
# Transformer(2017)
```
Attention Is All You Need[Taransformer]
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
(Submitted on 12 Jun 2017 (v1), last revised 6 Dec 2017 (this version, v5))
https://arxiv.org/abs/1706.03762
```
```
Layer normalization
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 
arXiv:1607.06450, 2016
```
```
不同预训练模型的总结对比
持续更新 2020-06-28
```
# BERT[2018]
```
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
https://arxiv.org/abs/1810.04805
```
```
A Primer in BERTology: What we know about how BERT works
Anna Rogers, Olga Kovaleva, Anna Rumshisky
https://arxiv.org/abs/2002.12327
```
## 各式各樣的BERT
```
Wenhao Lu, Jian Jiao, and Ruofei Zhang. 
TwinBERT: Distilling knowledge to twin-structured BERT models for efficient retrieval. 
arXiv preprint arXiv:2002.06275, 2020.
```
```
Benjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann. 2019. 
exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformers Models. 
arXiv:1910.05276
```
```
SpanBERT: Improving Pre-training by Representing and Predicting Spans
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, Omer Levy
https://arxiv.org/abs/1907.10529
```
```
CamemBERT: a Tasty French Language Model
Louis Martin, Benjamin Muller, Pedro Javier Ortiz Suárez, Yoann Dupont, Laurent Romary, 
Éric Villemonte de la Clergerie, Djamé Seddah, Benoît Sagot
https://arxiv.org/abs/1911.03894
```
# Google T5(2019)
```
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu
https://arxiv.org/abs/1910.10683
```
# GPT  1,2,3
```
GPT-1[2018]
Improving Language Understanding by Generative Pre-Training
https://pdfs.semanticscholar.org/cd18/800a0fe0b668a1cc19f2ec95b5003d0a5035.pdf?_ga=2.19061950.70118991.1595022018-1533186678.1594848993
```

```
GPT-2[2019]
Language Models are Unsupervised Multitask Learners
https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
```

```
GPT-3[2020]
Language Models are Few-Shot Learners
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, 
Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, 
Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, 
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei
https://arxiv.org/abs/2005.14165
```
# 2020
```
[微軟團隊]
UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training
Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Songhao Piao, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon
(Submitted on 28 Feb 2020)

https://arxiv.org/abs/2002.12804
https://github.com/microsoft/unilm
```

```
UniViLM: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation
Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Xilin Chen, Ming Zhou
(Submitted on 15 Feb 2020)
https://arxiv.org/abs/2002.06353
```

#
```
AdvCodec: Towards A Unified Framework for Adversarial Text Generation
Boxin Wang, Hengzhi Pei, Han Liu, Bo Li
(Submitted on 22 Dec 2019)
https://arxiv.org/abs/1912.10375
```



```

```
