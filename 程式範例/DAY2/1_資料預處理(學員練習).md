# 1.Token 法Tokenization
```
from tensorflow.keras.preprocessing.text import Tokenizer

samples = [ '吃什麼？', '咖哩飯 還是 牛排 還是 麵包 還是 都 吃']

tokenizer =Tokenizer()
tokenizer.fit_on_texts(samples)

print(tokenizer.word_counts)
print(tokenizer.index_word)
print(tokenizer.word_index)

seq= tokenizer.texts_to_sequences([吃 咖哩飯','牛排 和 麵包'])
pr1nt(seq)

seq2= tokenizer.texts_to_sequences([['吃', '咖哩飯'],['牛排', '和','麵包']])
print(seq2)

text= tokenizer.sequences_to_texts(seq)
print(text)
``
```
samples =['吃 什麼？','咖哩飯 還是 牛排 還是 麵包 還是 都 吃']
tokenizer = Tokenizer(num_words=3)
tokenizer.fit_on_texts(samples)
print(tokenizer.index_word)

seq= tokenizer.texts_to_sequences(['吃 咖哩飯 和 麵包'])
print(seq)
```
```
samples =['吃 什麼？','咖哩飯 還是 牛排 還是 麵包 還是 都 吃']
tokenizer = Tokenizer(num_words=3,oov_token= 'N')
tokenizer.fit_on_texts(samples)
print(tokenizer.index_word)

seq= tokenizer.texts_to_sequences(['吃 咖哩飯 和 麵包 和 牛奶'])
print(seq)

text= tokenizer.sequences_to_texts(seq)
print(text)
```

# 2.Seq padding
```
from tensorflow.keras.preprocessing.sequence import pad_sequences

seq= [[5,1], [2], [3, 5, 4, 7, 8]] 

pad_seq = pad_sequences(seq, maxlen=4, dtype= 'int32', padding='pre',truncating='post', value=0) 

print(pad_seq)
```

# 3.one-hot encoding
```
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

num_classes = 10 
convsat =['你好 哈囉',
'你是誰 不告訴 你',
'你 會做 什麼 都不會',
'什麼是 雞蛋 不知道']

tokenizer = Tokenizer(num_words = num_classes, oov_token='N')

tokenizer.fit_on_texts(convsat)

convsat_seq = tokenizer.texts_to_sequences(convsat) 

convsat_seq = pad_sequences(convsat_seq)
print(convsat_seq) 

convsat_one_hot = to_categorical(convsat_seq, num_classes=num_classes) 
print(convsat_one_hot)
```

# 4.Multi-hot encoding
```
from tensorflow.keras.preprocessing.text import Tokenizer

comment= ['真 好看 我 喜歡',
 '還好 缺乏 一點 劇情',
'有 一點 雷 個人 不 喜歡',
'我 喜歡 但 我 朋友 都 不 愛']

tokenizer = Tokenizer(num_words=10) 

tokenizer.fit_on_texts(comment) 

comment_multi_hot= tokenizer.texts_to_matrix(comment, mode='binary')
print(comment_multi_hot)

comment_multi_hot2= tokenizer.texts_to_matrix(comment, mode= 'count' )
print(comment_multi_hot2)
```

# word-embbeding
```

```


`
