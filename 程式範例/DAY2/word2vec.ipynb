{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15"
    },
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9hVCxucg85X",
        "colab_type": "text"
      },
      "source": [
        "# Word2Vec (Word Embedding)\n",
        "\n",
        "Implement Word2Vec algorithm to compute vector representations of words, with TensorFlow 2.0. This example is using a small chunk of Wikipedia articles to train from.\n",
        "\n",
        "More info: [Mikolov, Tomas et al. \"Efficient Estimation of Word Representations in Vector Space.\", 2013](https://arxiv.org/pdf/1301.3781.pdf)\n",
        "\n",
        "- Author: Aymeric Damien\n",
        "- Project: https://github.com/aymericdamien/TensorFlow-Examples/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRFxBPdKg85Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division, print_function, absolute_import\n",
        "\n",
        "import collections\n",
        "import os\n",
        "import random\n",
        "import urllib\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WNTJAQHg85g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Parameters.\n",
        "learning_rate = 0.1\n",
        "batch_size = 128\n",
        "num_steps = 3000000\n",
        "display_step = 10000\n",
        "eval_step = 200000\n",
        "\n",
        "# Evaluation Parameters.\n",
        "eval_words = ['five', 'of', 'going', 'hardware', 'american', 'britain']\n",
        "\n",
        "# Word2Vec Parameters.\n",
        "embedding_size = 200 # Dimension of the embedding vector.\n",
        "max_vocabulary_size = 50000 # Total number of different words in the vocabulary.\n",
        "min_occurrence = 10 # Remove all words that does not appears at least n times.\n",
        "skip_window = 3 # How many words to consider left and right.\n",
        "num_skips = 2 # How many times to reuse an input to generate a label.\n",
        "num_sampled = 64 # Number of negative examples to sample."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfY-Za6Qg85n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6a42ac82-cc91-49a5-a380-385eb08ea816"
      },
      "source": [
        "# Download a small chunk of Wikipedia articles collection.\n",
        "url = 'http://mattmahoney.net/dc/text8.zip'\n",
        "data_path = 'text8.zip'\n",
        "if not os.path.exists(data_path):\n",
        "    print(\"Downloading the dataset... (It may take some time)\")\n",
        "    filename, _ = urllib.urlretrieve(url, data_path)\n",
        "    print(\"Done!\")\n",
        "# Unzip the dataset file. Text has already been processed.\n",
        "with zipfile.ZipFile(data_path) as f:\n",
        "    text_words = f.read(f.namelist()[0]).lower().split()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading the dataset... (It may take some time)\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqwJ3BrFg85x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "534c60f0-0d21-4918-aae7-94952021d320"
      },
      "source": [
        "# Build the dictionary and replace rare words with UNK token.\n",
        "count = [('UNK', -1)]\n",
        "# Retrieve the most common words.\n",
        "count.extend(collections.Counter(text_words).most_common(max_vocabulary_size - 1))\n",
        "# Remove samples with less than 'min_occurrence' occurrences.\n",
        "for i in range(len(count) - 1, -1, -1):\n",
        "    if count[i][1] < min_occurrence:\n",
        "        count.pop(i)\n",
        "    else:\n",
        "        # The collection is ordered, so stop when 'min_occurrence' is reached.\n",
        "        break\n",
        "# Compute the vocabulary size.\n",
        "vocabulary_size = len(count)\n",
        "# Assign an id to each word.\n",
        "word2id = dict()\n",
        "for i, (word, _)in enumerate(count):\n",
        "    word2id[word] = i\n",
        "\n",
        "data = list()\n",
        "unk_count = 0\n",
        "for word in text_words:\n",
        "    # Retrieve a word id, or assign it index 0 ('UNK') if not in dictionary.\n",
        "    index = word2id.get(word, 0)\n",
        "    if index == 0:\n",
        "        unk_count += 1\n",
        "    data.append(index)\n",
        "count[0] = ('UNK', unk_count)\n",
        "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
        "\n",
        "print(\"Words count:\", len(text_words))\n",
        "print(\"Unique words:\", len(set(text_words)))\n",
        "print(\"Vocabulary size:\", vocabulary_size)\n",
        "print(\"Most common words:\", count[:10])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count: 17005207\n",
            "Unique words: 253854\n",
            "Vocabulary size: 47135\n",
            "Most common words: [('UNK', 444176), ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764), ('in', 372201), ('a', 325873), ('to', 316376), ('zero', 264975), ('nine', 250430)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc3LO5bIg85-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_index = 0\n",
        "# Generate training batch for the skip-gram model.\n",
        "def next_batch(batch_size, num_skips, skip_window):\n",
        "    global data_index\n",
        "    assert batch_size % num_skips == 0\n",
        "    assert num_skips <= 2 * skip_window\n",
        "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    # get window size (words left and right + current one).\n",
        "    span = 2 * skip_window + 1\n",
        "    buffer = collections.deque(maxlen=span)\n",
        "    if data_index + span > len(data):\n",
        "        data_index = 0\n",
        "    buffer.extend(data[data_index:data_index + span])\n",
        "    data_index += span\n",
        "    for i in range(batch_size // num_skips):\n",
        "        context_words = [w for w in range(span) if w != skip_window]\n",
        "        words_to_use = random.sample(context_words, num_skips)\n",
        "        for j, context_word in enumerate(words_to_use):\n",
        "            batch[i * num_skips + j] = buffer[skip_window]\n",
        "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
        "        if data_index == len(data):\n",
        "            buffer.extend(data[0:span])\n",
        "            data_index = span\n",
        "        else:\n",
        "            buffer.append(data[data_index])\n",
        "            data_index += 1\n",
        "    # Backtrack a little bit to avoid skipping words in the end of a batch.\n",
        "    data_index = (data_index + len(data) - span) % len(data)\n",
        "    return batch, labels"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW1yyagHg86H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ensure the following ops & var are assigned on CPU\n",
        "# (some ops are not compatible on GPU).\n",
        "with tf.device('/cpu:0'):\n",
        "    # Create the embedding variable (each row represent a word embedding vector).\n",
        "    embedding = tf.Variable(tf.random.normal([vocabulary_size, embedding_size]))\n",
        "    # Construct the variables for the NCE loss.\n",
        "    nce_weights = tf.Variable(tf.random.normal([vocabulary_size, embedding_size]))\n",
        "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "\n",
        "def get_embedding(x):\n",
        "    with tf.device('/cpu:0'):\n",
        "        # Lookup the corresponding embedding vectors for each sample in X.\n",
        "        x_embed = tf.nn.embedding_lookup(embedding, x)\n",
        "        return x_embed\n",
        "\n",
        "def nce_loss(x_embed, y):\n",
        "    with tf.device('/cpu:0'):\n",
        "        # Compute the average NCE loss for the batch.\n",
        "        y = tf.cast(y, tf.int64)\n",
        "        loss = tf.reduce_mean(\n",
        "            tf.nn.nce_loss(weights=nce_weights,\n",
        "                           biases=nce_biases,\n",
        "                           labels=y,\n",
        "                           inputs=x_embed,\n",
        "                           num_sampled=num_sampled,\n",
        "                           num_classes=vocabulary_size))\n",
        "        return loss\n",
        "\n",
        "# Evaluation.\n",
        "def evaluate(x_embed):\n",
        "    with tf.device('/cpu:0'):\n",
        "        # Compute the cosine similarity between input data embedding and every embedding vectors\n",
        "        x_embed = tf.cast(x_embed, tf.float32)\n",
        "        x_embed_norm = x_embed / tf.sqrt(tf.reduce_sum(tf.square(x_embed)))\n",
        "        embedding_norm = embedding / tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keepdims=True), tf.float32)\n",
        "        cosine_sim_op = tf.matmul(x_embed_norm, embedding_norm, transpose_b=True)\n",
        "        return cosine_sim_op\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.optimizers.SGD(learning_rate)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmGqcJ5dg86P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimization process. \n",
        "def run_optimization(x, y):\n",
        "    with tf.device('/cpu:0'):\n",
        "        # Wrap computation inside a GradientTape for automatic differentiation.\n",
        "        with tf.GradientTape() as g:\n",
        "            emb = get_embedding(x)\n",
        "            loss = nce_loss(emb, y)\n",
        "\n",
        "        # Compute gradients.\n",
        "        gradients = g.gradient(loss, [embedding, nce_weights, nce_biases])\n",
        "\n",
        "        # Update W and b following gradients.\n",
        "        optimizer.apply_gradients(zip(gradients, [embedding, nce_weights, nce_biases]))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "FBItCR6ng86V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4e167fc7-077e-49eb-fbc2-2b91f7a0b1a9"
      },
      "source": [
        "# Words for testing.\n",
        "x_test = np.array([word2id[w] for w in eval_words])\n",
        "\n",
        "# Run training for the given number of steps.\n",
        "for step in xrange(1, num_steps + 1):\n",
        "    batch_x, batch_y = next_batch(batch_size, num_skips, skip_window)\n",
        "    run_optimization(batch_x, batch_y)\n",
        "    \n",
        "    if step % display_step == 0 or step == 1:\n",
        "        loss = nce_loss(get_embedding(batch_x), batch_y)\n",
        "        print(\"step: %i, loss: %f\" % (step, loss))\n",
        "        \n",
        "    # Evaluation.\n",
        "    if step % eval_step == 0 or step == 1:\n",
        "        print(\"Evaluation...\")\n",
        "        sim = evaluate(get_embedding(x_test)).numpy()\n",
        "        for i in xrange(len(eval_words)):\n",
        "            top_k = 8  # number of nearest neighbors.\n",
        "            nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
        "            log_str = '\"%s\" nearest neighbors:' % eval_words[i]\n",
        "            for k in xrange(top_k):\n",
        "                log_str = '%s %s,' % (log_str, id2word[nearest[k]])\n",
        "            print(log_str)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 1, loss: 516.350708\n",
            "Evaluation...\n",
            "\"five\" nearest neighbors: tutte, sanctioned, jess, bokassa, juveniles, conglomerates, concourse, arrian,\n",
            "\"of\" nearest neighbors: countercult, haemophilia, goldwater, juice, amoeboids, posteriori, aegina, bosman,\n",
            "\"going\" nearest neighbors: uns, indented, unnecessarily, line, bahia, codification, diving, repairing,\n",
            "\"hardware\" nearest neighbors: receptor, sixties, yi, whimsical, offa, affective, coming, paws,\n",
            "\"american\" nearest neighbors: accredited, discography, korner, wavelet, jamaica, carolingian, composites, ayla,\n",
            "\"britain\" nearest neighbors: lowlands, infer, intervene, adventurer, winger, followers, evas, dalmatia,\n",
            "step: 10000, loss: 152.995331\n",
            "step: 20000, loss: 74.387337\n",
            "step: 30000, loss: 44.425720\n",
            "step: 40000, loss: 18.882523\n",
            "step: 50000, loss: 48.086723\n",
            "step: 60000, loss: 42.244743\n",
            "step: 70000, loss: 16.802782\n",
            "step: 80000, loss: 25.816784\n",
            "step: 90000, loss: 48.029446\n",
            "step: 100000, loss: 46.296150\n",
            "step: 110000, loss: 26.517788\n",
            "step: 120000, loss: 17.334038\n",
            "step: 130000, loss: 22.773264\n",
            "step: 140000, loss: 25.277243\n",
            "step: 150000, loss: 10.690424\n",
            "step: 160000, loss: 23.444149\n",
            "step: 170000, loss: 14.147614\n",
            "step: 180000, loss: 19.223076\n",
            "step: 190000, loss: 10.070961\n",
            "step: 200000, loss: 18.208818\n",
            "Evaluation...\n",
            "\"five\" nearest neighbors: three, four, eight, six, seven, two, nine, zero,\n",
            "\"of\" nearest neighbors: the, first, and, from, with, which, when, a,\n",
            "\"going\" nearest neighbors: line, known, change, only, can, it, first, to,\n",
            "\"hardware\" nearest neighbors: one, zero, four, seven, six, two, three, continued,\n",
            "\"american\" nearest neighbors: UNK, and, from, s, see, when, in, after,\n",
            "\"britain\" nearest neighbors: war, some, was, other, only, or, most, were,\n",
            "step: 210000, loss: 13.945910\n",
            "step: 220000, loss: 12.211603\n",
            "step: 230000, loss: 17.109032\n",
            "step: 240000, loss: 6.451307\n",
            "step: 250000, loss: 10.212020\n",
            "step: 260000, loss: 11.343449\n",
            "step: 270000, loss: 10.600195\n",
            "step: 280000, loss: 6.925337\n",
            "step: 290000, loss: 9.957182\n",
            "step: 300000, loss: 11.610666\n",
            "step: 310000, loss: 11.947260\n",
            "step: 320000, loss: 10.510521\n",
            "step: 330000, loss: 7.185376\n",
            "step: 340000, loss: 9.195421\n",
            "step: 350000, loss: 16.915375\n",
            "step: 360000, loss: 8.725594\n",
            "step: 370000, loss: 6.498474\n",
            "step: 380000, loss: 7.495565\n",
            "step: 390000, loss: 8.913212\n",
            "step: 400000, loss: 8.234125\n",
            "Evaluation...\n",
            "\"five\" nearest neighbors: four, three, six, seven, eight, one, two, nine,\n",
            "\"of\" nearest neighbors: with, the, and, for, its, a, on, or,\n",
            "\"going\" nearest neighbors: known, form, system, usually, while, line, it, may,\n",
            "\"hardware\" nearest neighbors: e, during, s, time, university, c, under, since,\n",
            "\"american\" nearest neighbors: british, s, in, at, and, after, history, since,\n",
            "\"britain\" nearest neighbors: war, name, those, her, were, had, number, has,\n",
            "step: 410000, loss: 7.232174\n",
            "step: 420000, loss: 16.533611\n",
            "step: 430000, loss: 10.267972\n",
            "step: 440000, loss: 7.823050\n",
            "step: 450000, loss: 6.685635\n",
            "step: 460000, loss: 11.770510\n",
            "step: 470000, loss: 7.080447\n",
            "step: 480000, loss: 25.365456\n",
            "step: 490000, loss: 6.515299\n",
            "step: 500000, loss: 9.088690\n",
            "step: 510000, loss: 8.989944\n",
            "step: 520000, loss: 9.210786\n",
            "step: 530000, loss: 8.741325\n",
            "step: 540000, loss: 7.347355\n",
            "step: 550000, loss: 12.782604\n",
            "step: 560000, loss: 7.458018\n",
            "step: 570000, loss: 7.083132\n",
            "step: 580000, loss: 7.634330\n",
            "step: 590000, loss: 7.466613\n",
            "step: 600000, loss: 5.983275\n",
            "Evaluation...\n",
            "\"five\" nearest neighbors: four, three, six, seven, two, eight, one, zero,\n",
            "\"of\" nearest neighbors: the, its, and, first, for, in, including, a,\n",
            "\"going\" nearest neighbors: line, made, second, before, however, it, to, usually,\n",
            "\"hardware\" nearest neighbors: UNK, university, but, and, using, during, early, by,\n",
            "\"american\" nearest neighbors: english, german, french, british, s, john, including, early,\n",
            "\"britain\" nearest neighbors: his, her, within, of, being, high, war, see,\n",
            "step: 610000, loss: 3.572489\n",
            "step: 620000, loss: 5.328915\n",
            "step: 630000, loss: 6.560250\n",
            "step: 640000, loss: 9.485731\n",
            "step: 650000, loss: 7.635569\n",
            "step: 660000, loss: 6.871999\n",
            "step: 670000, loss: 5.711724\n",
            "step: 680000, loss: 5.270271\n",
            "step: 690000, loss: 7.688128\n",
            "step: 700000, loss: 7.879439\n",
            "step: 710000, loss: 11.720243\n",
            "step: 720000, loss: 5.571039\n",
            "step: 730000, loss: 8.390364\n",
            "step: 740000, loss: 7.917736\n",
            "step: 750000, loss: 12.741362\n",
            "step: 760000, loss: 9.286805\n",
            "step: 770000, loss: 9.373903\n",
            "step: 780000, loss: 8.278862\n",
            "step: 790000, loss: 7.789409\n",
            "step: 800000, loss: 6.392796\n",
            "Evaluation...\n",
            "\"five\" nearest neighbors: six, four, three, seven, eight, two, one, nine,\n",
            "\"of\" nearest neighbors: and, including, the, at, from, with, see, its,\n",
            "\"going\" nearest neighbors: made, only, line, control, point, what, will, also,\n",
            "\"hardware\" nearest neighbors: members, called, including, later, free, day, on, by,\n",
            "\"american\" nearest neighbors: born, british, english, french, b, john, d, german,\n",
            "\"britain\" nearest neighbors: her, was, within, and, into, series, his, including,\n",
            "step: 810000, loss: 6.507936\n",
            "step: 820000, loss: 6.222463\n",
            "step: 830000, loss: 6.379485\n",
            "step: 840000, loss: 5.890118\n",
            "step: 850000, loss: 8.181499\n",
            "step: 860000, loss: 6.144885\n",
            "step: 870000, loss: 5.967085\n",
            "step: 880000, loss: 8.422976\n",
            "step: 890000, loss: 7.046424\n",
            "step: 900000, loss: 5.923961\n",
            "step: 910000, loss: 6.408288\n",
            "step: 920000, loss: 5.885909\n",
            "step: 930000, loss: 7.021845\n",
            "step: 940000, loss: 5.875862\n",
            "step: 950000, loss: 9.666349\n",
            "step: 960000, loss: 5.937604\n",
            "step: 970000, loss: 7.185714\n",
            "step: 980000, loss: 4.901192\n",
            "step: 990000, loss: 7.271944\n",
            "step: 1000000, loss: 4.800769\n",
            "Evaluation...\n",
            "\"five\" nearest neighbors: six, three, four, seven, eight, two, nine, zero,\n",
            "\"of\" nearest neighbors: the, and, first, became, from, following, s, by,\n",
            "\"going\" nearest neighbors: made, this, control, line, make, while, a, change,\n",
            "\"hardware\" nearest neighbors: members, using, see, under, free, each, music, own,\n",
            "\"american\" nearest neighbors: d, b, born, UNK, nine, john, one, french,\n",
            "\"britain\" nearest neighbors: her, with, took, especially, year, other, when, former,\n",
            "step: 1010000, loss: 5.970522\n",
            "step: 1020000, loss: 4.891084\n",
            "step: 1030000, loss: 9.183816\n",
            "step: 1040000, loss: 5.763487\n",
            "step: 1050000, loss: 7.477591\n",
            "step: 1060000, loss: 5.354149\n",
            "step: 1070000, loss: 8.699736\n",
            "step: 1080000, loss: 6.792254\n",
            "step: 1090000, loss: 5.877833\n",
            "step: 1100000, loss: 6.380654\n",
            "step: 1110000, loss: 7.794771\n",
            "step: 1120000, loss: 6.454856\n",
            "step: 1130000, loss: 7.651492\n",
            "step: 1140000, loss: 5.948436\n",
            "step: 1150000, loss: 6.236772\n",
            "step: 1160000, loss: 5.851512\n",
            "step: 1170000, loss: 8.613521\n",
            "step: 1180000, loss: 6.297613\n",
            "step: 1190000, loss: 6.381794\n",
            "step: 1200000, loss: 5.434556\n",
            "Evaluation...\n",
            "\"five\" nearest neighbors: four, six, three, seven, eight, two, nine, zero,\n",
            "\"of\" nearest neighbors: in, and, the, including, following, from, its, on,\n",
            "\"going\" nearest neighbors: making, only, before, special, every, change, control, another,\n",
            "\"hardware\" nearest neighbors: members, music, all, before, time, each, using, at,\n",
            "\"american\" nearest neighbors: born, john, german, UNK, d, british, b, french,\n",
            "\"britain\" nearest neighbors: took, former, after, established, especially, first, her, later,\n",
            "step: 1210000, loss: 6.577971\n",
            "step: 1220000, loss: 5.778967\n",
            "step: 1230000, loss: 6.292551\n",
            "step: 1240000, loss: 6.453337\n",
            "step: 1250000, loss: 6.370265\n",
            "step: 1260000, loss: 7.941365\n",
            "step: 1270000, loss: 5.757340\n",
            "step: 1280000, loss: 8.970820\n",
            "step: 1290000, loss: 5.686887\n",
            "step: 1300000, loss: 6.744034\n",
            "step: 1310000, loss: 5.433410\n",
            "step: 1320000, loss: 4.777259\n",
            "step: 1330000, loss: 5.805717\n",
            "step: 1340000, loss: 5.272593\n",
            "step: 1350000, loss: 6.412733\n",
            "step: 1360000, loss: 8.571079\n",
            "step: 1370000, loss: 5.206866\n",
            "step: 1380000, loss: 5.610071\n",
            "step: 1390000, loss: 6.107656\n",
            "step: 1400000, loss: 7.680105\n",
            "Evaluation...\n",
            "\"five\" nearest neighbors: three, four, six, two, seven, eight, one, zero,\n",
            "\"of\" nearest neighbors: the, and, within, including, modern, its, in, for,\n",
            "\"going\" nearest neighbors: making, to, before, every, special, only, back, made,\n",
            "\"hardware\" nearest neighbors: members, created, all, non, free, using, for, under,\n",
            "\"american\" nearest neighbors: english, french, german, born, british, john, s, references,\n",
            "\"britain\" nearest neighbors: established, europe, later, former, took, within, in, among,\n",
            "step: 1410000, loss: 7.209009\n",
            "step: 1420000, loss: 4.523329\n",
            "step: 1430000, loss: 5.240423\n",
            "step: 1440000, loss: 6.147555\n",
            "step: 1450000, loss: 5.238991\n",
            "step: 1460000, loss: 5.636275\n",
            "step: 1470000, loss: 6.242653\n",
            "step: 1480000, loss: 6.009103\n",
            "step: 1490000, loss: 5.086113\n",
            "step: 1500000, loss: 5.460148\n",
            "step: 1510000, loss: 6.345368\n",
            "step: 1520000, loss: 5.753166\n",
            "step: 1530000, loss: 6.391370\n",
            "step: 1540000, loss: 5.900384\n",
            "step: 1550000, loss: 6.123816\n",
            "step: 1560000, loss: 5.546020\n",
            "step: 1570000, loss: 5.963325\n",
            "step: 1580000, loss: 4.633483\n",
            "step: 1590000, loss: 6.349198\n",
            "step: 1600000, loss: 5.580717\n",
            "Evaluation...\n",
            "\"five\" nearest neighbors: three, four, six, two, eight, seven, one, zero,\n",
            "\"of\" nearest neighbors: and, from, including, in, the, modern, a, with,\n",
            "\"going\" nearest neighbors: making, made, to, every, special, back, up, any,\n",
            "\"hardware\" nearest neighbors: terms, free, created, non, members, included, of, music,\n",
            "\"american\" nearest neighbors: born, english, british, french, german, actor, john, david,\n",
            "\"britain\" nearest neighbors: established, former, europe, western, region, after, during, especially,\n",
            "step: 1610000, loss: 6.183835\n",
            "step: 1620000, loss: 5.196332\n",
            "step: 1630000, loss: 5.525321\n",
            "step: 1640000, loss: 5.256860\n",
            "step: 1650000, loss: 7.131527\n",
            "step: 1660000, loss: 6.280930\n",
            "step: 1670000, loss: 4.941460\n",
            "step: 1680000, loss: 5.373648\n",
            "step: 1690000, loss: 5.338397\n",
            "step: 1700000, loss: 7.748185\n",
            "step: 1710000, loss: 5.974450\n",
            "step: 1720000, loss: 5.573770\n",
            "step: 1730000, loss: 5.863638\n",
            "step: 1740000, loss: 5.994910\n",
            "step: 1750000, loss: 6.065701\n",
            "step: 1760000, loss: 12.011730\n",
            "step: 1770000, loss: 5.271422\n",
            "step: 1780000, loss: 5.654586\n",
            "step: 1790000, loss: 4.748431\n",
            "step: 1800000, loss: 7.443089\n",
            "Evaluation...\n",
            "\"five\" nearest neighbors: three, six, four, seven, eight, two, nine, zero,\n",
            "\"of\" nearest neighbors: the, first, from, following, at, in, became, and,\n",
            "\"going\" nearest neighbors: making, only, made, this, special, would, given, upon,\n",
            "\"hardware\" nearest neighbors: terms, free, each, members, every, all, created, specific,\n",
            "\"american\" nearest neighbors: actor, d, b, born, UNK, john, david, robert,\n",
            "\"britain\" nearest neighbors: established, europe, former, western, northern, great, from, during,\n",
            "step: 1810000, loss: 5.592821\n",
            "step: 1820000, loss: 5.293673\n",
            "step: 1830000, loss: 7.408010\n",
            "step: 1840000, loss: 5.196899\n",
            "step: 1850000, loss: 5.524901\n",
            "step: 1860000, loss: 6.320116\n",
            "step: 1870000, loss: 6.060765\n",
            "step: 1880000, loss: 5.512319\n",
            "step: 1890000, loss: 5.343516\n",
            "step: 1900000, loss: 5.198976\n",
            "step: 1910000, loss: 6.208023\n",
            "step: 1920000, loss: 5.197319\n",
            "step: 1930000, loss: 6.837013\n",
            "step: 1940000, loss: 5.147947\n",
            "step: 1950000, loss: 5.765250\n",
            "step: 1960000, loss: 6.712281\n",
            "step: 1970000, loss: 4.898246\n",
            "step: 1980000, loss: 6.004687\n",
            "step: 1990000, loss: 6.879700\n",
            "step: 2000000, loss: 6.471716\n",
            "Evaluation...\n",
            "\"five\" nearest neighbors: four, three, six, seven, eight, two, nine, zero,\n",
            "\"of\" nearest neighbors: the, and, in, following, modern, from, including, through,\n",
            "\"going\" nearest neighbors: making, this, before, only, special, to, how, made,\n",
            "\"hardware\" nearest neighbors: terms, each, all, free, results, music, every, non,\n",
            "\"american\" nearest neighbors: actor, born, italian, john, d, b, writer, william,\n",
            "\"britain\" nearest neighbors: europe, former, established, western, northern, germany, england, leader,\n",
            "step: 2010000, loss: 5.380996\n",
            "step: 2020000, loss: 5.504087\n",
            "step: 2030000, loss: 5.517990\n",
            "step: 2040000, loss: 6.200393\n",
            "step: 2050000, loss: 5.142128\n",
            "step: 2060000, loss: 4.295347\n",
            "step: 2070000, loss: 4.986182\n",
            "step: 2080000, loss: 4.544661\n",
            "step: 2090000, loss: 4.499515\n",
            "step: 2100000, loss: 5.273110\n",
            "step: 2110000, loss: 5.910732\n",
            "step: 2120000, loss: 6.857998\n",
            "step: 2130000, loss: 5.125485\n",
            "step: 2140000, loss: 6.944810\n",
            "step: 2150000, loss: 5.535927\n",
            "step: 2160000, loss: 5.211500\n",
            "step: 2170000, loss: 5.186564\n",
            "step: 2180000, loss: 6.000518\n",
            "step: 2190000, loss: 5.057541\n",
            "step: 2200000, loss: 6.681192\n",
            "Evaluation...\n",
            "\"five\" nearest neighbors: four, three, six, seven, two, eight, one, zero,\n",
            "\"of\" nearest neighbors: the, and, in, including, group, modern, original, with,\n",
            "\"going\" nearest neighbors: making, made, may, before, to, what, back, that,\n",
            "\"hardware\" nearest neighbors: terms, results, specific, free, every, non, of, key,\n",
            "\"american\" nearest neighbors: canadian, french, english, italian, german, british, author, born,\n",
            "\"britain\" nearest neighbors: europe, established, western, former, northern, germany, england, throughout,\n",
            "step: 2210000, loss: 4.525193\n",
            "step: 2220000, loss: 6.102368\n",
            "step: 2230000, loss: 5.341857\n",
            "step: 2240000, loss: 6.975045\n",
            "step: 2250000, loss: 5.541903\n",
            "step: 2260000, loss: 5.215517\n",
            "step: 2270000, loss: 8.690922\n",
            "step: 2280000, loss: 5.518820\n",
            "step: 2290000, loss: 5.427997\n",
            "step: 2300000, loss: 5.290491\n",
            "step: 2310000, loss: 6.268841\n",
            "step: 2320000, loss: 5.795774\n",
            "step: 2330000, loss: 6.160369\n",
            "step: 2340000, loss: 5.739942\n",
            "step: 2350000, loss: 6.890159\n",
            "step: 2360000, loss: 5.438189\n",
            "step: 2370000, loss: 5.929813\n",
            "step: 2380000, loss: 5.947778\n",
            "step: 2390000, loss: 5.831479\n",
            "step: 2400000, loss: 6.667229\n",
            "Evaluation...\n",
            "\"five\" nearest neighbors: three, four, six, seven, two, eight, zero, one,\n",
            "\"of\" nearest neighbors: the, included, in, original, and, including, from, following,\n",
            "\"going\" nearest neighbors: made, making, back, to, this, take, taken, must,\n",
            "\"hardware\" nearest neighbors: results, specific, free, terms, run, using, key, included,\n",
            "\"american\" nearest neighbors: actor, author, canadian, born, italian, writer, english, d,\n",
            "\"britain\" nearest neighbors: europe, northern, established, western, former, germany, england, throughout,\n",
            "step: 2410000, loss: 6.039230\n",
            "step: 2420000, loss: 5.576550\n",
            "step: 2430000, loss: 5.185639\n",
            "step: 2440000, loss: 5.515628\n",
            "step: 2450000, loss: 5.441827\n",
            "step: 2460000, loss: 4.873871\n",
            "step: 2470000, loss: 6.426410\n",
            "step: 2480000, loss: 5.464257\n",
            "step: 2490000, loss: 5.054790\n",
            "step: 2500000, loss: 6.394045\n",
            "step: 2510000, loss: 5.288678\n",
            "step: 2520000, loss: 5.480668\n",
            "step: 2530000, loss: 6.341100\n",
            "step: 2540000, loss: 5.198052\n",
            "step: 2550000, loss: 5.045276\n",
            "step: 2560000, loss: 5.544833\n",
            "step: 2570000, loss: 5.563672\n",
            "step: 2580000, loss: 5.238031\n",
            "step: 2590000, loss: 6.016040\n",
            "step: 2600000, loss: 5.570280\n",
            "Evaluation...\n",
            "\"five\" nearest neighbors: three, four, six, seven, eight, nine, two, zero,\n",
            "\"of\" nearest neighbors: the, and, from, became, following, at, society, first,\n",
            "\"going\" nearest neighbors: making, only, up, taken, back, made, before, when,\n",
            "\"hardware\" nearest neighbors: results, specific, free, terms, key, program, various, systems,\n",
            "\"american\" nearest neighbors: actor, b, d, writer, author, born, canadian, italian,\n",
            "\"britain\" nearest neighbors: europe, northern, established, former, germany, england, western, france,\n",
            "step: 2610000, loss: 7.933252\n",
            "step: 2620000, loss: 5.121280\n",
            "step: 2630000, loss: 5.401713\n",
            "step: 2640000, loss: 4.933008\n",
            "step: 2650000, loss: 5.422099\n",
            "step: 2660000, loss: 6.266365\n",
            "step: 2670000, loss: 6.526638\n",
            "step: 2680000, loss: 5.611573\n",
            "step: 2690000, loss: 5.691904\n",
            "step: 2700000, loss: 5.596833\n",
            "step: 2710000, loss: 5.517906\n",
            "step: 2720000, loss: 5.513536\n",
            "step: 2730000, loss: 6.056809\n",
            "step: 2740000, loss: 6.393930\n",
            "step: 2750000, loss: 4.903521\n",
            "step: 2760000, loss: 6.341355\n",
            "step: 2770000, loss: 5.892772\n",
            "step: 2780000, loss: 5.851191\n",
            "step: 2790000, loss: 5.239091\n",
            "step: 2800000, loss: 5.130492\n",
            "Evaluation...\n",
            "\"five\" nearest neighbors: four, six, three, seven, eight, two, nine, zero,\n",
            "\"of\" nearest neighbors: the, in, and, from, under, including, following, within,\n",
            "\"going\" nearest neighbors: making, when, made, before, taken, would, put, only,\n",
            "\"hardware\" nearest neighbors: specific, results, free, key, definition, systems, personal, terms,\n",
            "\"american\" nearest neighbors: actor, writer, canadian, born, italian, singer, author, john,\n",
            "\"britain\" nearest neighbors: europe, established, germany, england, northern, former, france, throughout,\n",
            "step: 2810000, loss: 5.491820\n",
            "step: 2820000, loss: 6.827436\n",
            "step: 2830000, loss: 7.400691\n",
            "step: 2840000, loss: 7.340425\n",
            "step: 2850000, loss: 5.681006\n",
            "step: 2860000, loss: 5.294091\n",
            "step: 2870000, loss: 6.351307\n",
            "step: 2880000, loss: 5.255478\n",
            "step: 2890000, loss: 5.186281\n",
            "step: 2900000, loss: 4.979488\n",
            "step: 2910000, loss: 5.134329\n",
            "step: 2920000, loss: 6.020952\n",
            "step: 2930000, loss: 4.770734\n",
            "step: 2940000, loss: 6.762325\n",
            "step: 2950000, loss: 4.557612\n",
            "step: 2960000, loss: 6.452808\n",
            "step: 2970000, loss: 5.866443\n",
            "step: 2980000, loss: 6.179622\n",
            "step: 2990000, loss: 5.162476\n",
            "step: 3000000, loss: 5.405252\n",
            "Evaluation...\n",
            "\"five\" nearest neighbors: four, three, six, seven, eight, two, zero, one,\n",
            "\"of\" nearest neighbors: the, and, in, modern, original, group, within, for,\n",
            "\"going\" nearest neighbors: making, put, made, taken, only, come, before, would,\n",
            "\"hardware\" nearest neighbors: specific, results, definition, key, using, computer, free, non,\n",
            "\"american\" nearest neighbors: canadian, english, british, irish, french, author, italian, german,\n",
            "\"britain\" nearest neighbors: germany, europe, established, northern, throughout, england, former, france,\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}