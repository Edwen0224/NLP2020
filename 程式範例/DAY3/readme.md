#
```



```

# seq2seq
```
Neural machine translation without attention
```
# attention
```
Neural machine translation with attention
https://tensorflow.google.cn/tutorials/text/nmt_with_attention
```

# Transformer model
```
Attention Is All You Need
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
https://arxiv.org/abs/1706.03762
```
```
Transformer model for language understanding
https://tensorflow.google.cn/tutorials/text/transformer
```
# Pre-Trained model
```
GPT 
```
# BERT(2018)
```
Devlin, J., Chang, M.-W., Lee, K., and Toutanova,K. (2019). 
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
arXiv:1810.04805 
```
```
Fine-tuning a BERT model
https://tensorflow.google.cn/official_models/fine_tuning_bert
```
