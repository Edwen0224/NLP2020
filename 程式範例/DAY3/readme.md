# 主題
```
Neural machine translation(NMT)
Question Answering 
```
# Topic 1:Neural machine translation(NMT)

### seq2seq(2014)
```
Sequence to Sequence Learning with Neural Networks
Ilya Sutskever, Oriol Vinyals, Quoc V. Le
https://arxiv.org/abs/1409.3215
```
```
Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio
https://arxiv.org/abs/1406.1078
```
```
Sequence-to-sequence Learning
https://www.youtube.com/watch?v=ZjfjPzXw6og
```
```
Neural machine translation without attention
```
### attention
```
Attention Is All You Need
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
https://arxiv.org/abs/1706.03762
```
```
Neural machine translation with attention
https://tensorflow.google.cn/tutorials/text/nmt_with_attention
```


### Transformer model(2017)
```
Attention Is All You Need
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
https://arxiv.org/abs/1706.03762
```
```
Transformer model for language understanding
https://tensorflow.google.cn/tutorials/text/transformer
```
```
Transformer
https://www.youtube.com/watch?v=ugWDIIOHtPA
```
```
淺談神經機器翻譯 & 用 Transformer 與 TensorFlow 2 英翻中
https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html
```
### ELMO(Embeddings from Language Models)(2018)
```
Matthew Peters et al., "Deep Contextualized Word Representations," Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies I (2018): 2227-2237.

Deep contextualized word representations
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer
https://arxiv.org/abs/1802.05365
```
```
ELMo原理解析及简单上手使用
https://zhuanlan.zhihu.com/p/51679783
```
```
ELMO, BERT, GPT
https://www.youtube.com/watch?v=UYPa347-DdE
```
### RoBERTa(2019)
```
RoBERTa: A Robustly Optimized BERT Pretraining Approach
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, 
Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov
https://arxiv.org/abs/1907.11692
```
### XLM(2019)
```
Cross-lingual Language Model Pretraining
Guillaume Lample, Alexis Conneau
https://arxiv.org/abs/1901.07291

```
### Megatron-LM(2019)
```
Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro
https://arxiv.org/abs/1909.08053

https://github.com/NVIDIA/Megatron-LM
```

### BERT(2018)
```
Devlin, J., Chang, M.-W., Lee, K., and Toutanova,K. (2019). 
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
arXiv:1810.04805 
```
```
Fine-tuning a BERT model
https://tensorflow.google.cn/official_models/fine_tuning_bert
```
```
進擊的 BERT：NLP 界的巨人之力與遷移學習
2019-07-10 (Wed) 48,073 views
https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html
```
```
http://mccormickml.com/tutorials/

Pytorch
BERT Application Examples:
Word Embeddings (post, notebook)
Sentence Classification (post, notebook)
Document Classification (video, notebook)
```
### BERT and its family
```
BERT and its family
http://speech.ee.ntu.edu.tw/~tlkagk/courses/DLHLP20/BERT%20train%20(v8).pdf
```
#### DistilBERT(2019)
```
DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf
https://arxiv.org/abs/1910.01108
```
```
Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT
https://medium.com/huggingface/distilbert-8cf3380435b5
```
```
Distilling from BERT论文笔记
https://zhuanlan.zhihu.com/p/73046023
```
### Multilingual BERT
```
How multilingual is Multilingual BERT?
Telmo Pires, Eva Schlinger, Dan Garrette
https://arxiv.org/abs/1906.01502
```
```
http://speech.ee.ntu.edu.tw/~tlkagk/courses/DLHLP20/Multi%20(v2).pdf
```
### GPT 1-2-3
```
直觀理解 GPT-2 語言模型並生成金庸武俠小說
https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html

讓 AI 寫點金庸：如何用 TensorFlow 2.0 及 TensorFlow.js 寫天龍八部
https://leemeng.tw/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html
```
### GPT-3(2020)
```
Language Models are Few-Shot Learners
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, 
Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, 
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, 
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, 
Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei

https://arxiv.org/abs/2005.14165
https://github.com/openai/gpt-3
```
```
[research paper review] GPT-3 : Language Models are Few-Shot Learners
https://www.youtube.com/watch?v=Mq97CF02sRY
```
```
http://speech.ee.ntu.edu.tw/~tlkagk/courses/DLHLP20/GPT3%20(v6).pdf

GPT-3: Language Models are Few-Shot Learners (Paper Explaine
https://www.youtube.com/watch?v=SY5PvZrJhLE
```
# Question Answering 
```

```
```
http://speech.ee.ntu.edu.tw/~tlkagk/courses_DLHLP20.html

[DLHLP 2020] Deep Learning for Question Answering (1/2) (重新上傳)
https://www.youtube.com/watch?v=gRfTfXCe3LA

https://www.youtube.com/watch?v=gRfTfXCe3LA
https://www.youtube.com/watch?v=h_Lptoq8spQ
```
