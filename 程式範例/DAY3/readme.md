# 主題
```
Neural machine translation(NMT)
Question Answering 
```
# Topic 1:Neural machine translation(NMT)

### seq2seq(2014)
```
Sequence to Sequence Learning with Neural Networks
Ilya Sutskever, Oriol Vinyals, Quoc V. Le
https://arxiv.org/abs/1409.3215
```
```
Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio
https://arxiv.org/abs/1406.1078
```
```
Sequence-to-sequence Learning
https://www.youtube.com/watch?v=ZjfjPzXw6og
```
```
Neural machine translation without attention
```
### attention
```
Attention Is All You Need
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
https://arxiv.org/abs/1706.03762
```
```
Neural machine translation with attention
https://tensorflow.google.cn/tutorials/text/nmt_with_attention
```


### Transformer model(2017)
```
Attention Is All You Need
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
https://arxiv.org/abs/1706.03762
```
```
Transformer model for language understanding
https://tensorflow.google.cn/tutorials/text/transformer
```
```
Transformer
https://www.youtube.com/watch?v=ugWDIIOHtPA
```
```
淺談神經機器翻譯 & 用 Transformer 與 TensorFlow 2 英翻中
https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html
```
### 大型語言模型
### ELMO(Embeddings from Language Models)(2018)
```
Matthew Peters et al., "Deep Contextualized Word Representations," Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies I (2018): 2227-2237.

Deep contextualized word representations
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer
https://arxiv.org/abs/1802.05365
```
```
ELMo原理解析及简单上手使用
https://zhuanlan.zhihu.com/p/51679783
```
```
ELMO, BERT, GPT
https://www.youtube.com/watch?v=UYPa347-DdE
```
### RoBERTa(2019)
```
RoBERTa: A Robustly Optimized BERT Pretraining Approach
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, 
Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov
https://arxiv.org/abs/1907.11692
```
### XLM(2019)
```
Cross-lingual Language Model Pretraining
Guillaume Lample, Alexis Conneau
https://arxiv.org/abs/1901.07291
```
### Megatron-LM(2019)
```
Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro
https://arxiv.org/abs/1909.08053

https://github.com/NVIDIA/Megatron-LM
```
### XLNet(2019)
```
XLNet: Generalized Autoregressive Pretraining for Language Understanding
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le
https://arxiv.org/abs/1906.08237
```
```
中文XLNet预训练模型
https://github.com/ymcui/Chinese-XLNet
```
### BERT(2018)
```
Devlin, J., Chang, M.-W., Lee, K., and Toutanova,K. (2019). 
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
arXiv:1810.04805 
```
```
Fine-tuning a BERT model
https://tensorflow.google.cn/official_models/fine_tuning_bert
```
```
進擊的 BERT：NLP 界的巨人之力與遷移學習
2019-07-10 (Wed) 48,073 views
https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html
```
```
http://mccormickml.com/tutorials/

Pytorch
BERT Application Examples:
Word Embeddings (post, notebook)
Sentence Classification (post, notebook)
Document Classification (video, notebook)
```
### BERT and its family
```
BERT and its family
http://speech.ee.ntu.edu.tw/~tlkagk/courses/DLHLP20/BERT%20train%20(v8).pdf
```
#### DistilBERT(2019)
```
DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf
https://arxiv.org/abs/1910.01108
```
```
Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT
https://medium.com/huggingface/distilbert-8cf3380435b5
```
```
Distilling from BERT论文笔记
https://zhuanlan.zhihu.com/p/73046023
```
### Multilingual BERT
```
How multilingual is Multilingual BERT?
Telmo Pires, Eva Schlinger, Dan Garrette
https://arxiv.org/abs/1906.01502
```
```
http://speech.ee.ntu.edu.tw/~tlkagk/courses/DLHLP20/Multi%20(v2).pdf
```
### GPT 1-2-3
```
直觀理解 GPT-2 語言模型並生成金庸武俠小說
https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html

讓 AI 寫點金庸：如何用 TensorFlow 2.0 及 TensorFlow.js 寫天龍八部
https://leemeng.tw/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html
```
### GPT-3(2020)
```
Language Models are Few-Shot Learners
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, 
Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, 
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, 
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, 
Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei

https://arxiv.org/abs/2005.14165
https://github.com/openai/gpt-3
```
```
[research paper review] GPT-3 : Language Models are Few-Shot Learners
https://www.youtube.com/watch?v=Mq97CF02sRY
```
```
http://speech.ee.ntu.edu.tw/~tlkagk/courses/DLHLP20/GPT3%20(v6).pdf

GPT-3: Language Models are Few-Shot Learners (Paper Explaine
https://www.youtube.com/watch?v=SY5PvZrJhLE
```

# Question Answering 
```

```
```
http://speech.ee.ntu.edu.tw/~tlkagk/courses_DLHLP20.html

[DLHLP 2020] Deep Learning for Question Answering (1/2) (重新上傳)
https://www.youtube.com/watch?v=gRfTfXCe3LA

https://www.youtube.com/watch?v=gRfTfXCe3LA
https://www.youtube.com/watch?v=h_Lptoq8spQ
```
# Fake Object Detection and Generation
```
Defending Against Neural Fake News
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi
https://arxiv.org/abs/1905.12616
```
```
使用NLP檢測和對抗AI生成的假新聞
磐创 AI 2020-06-14
https://blog.csdn.net/fendouaini/java/article/details/106744426

GROVER—A State-of-the-Art Defense against Neural Fake News
https://grover.allenai.org/

https://rowanzellers.com/grover/

https://github.com/rowanz/grover
```
```
arXiv:2007.03316  
Graph Neural Networks with Continual Learning for Fake News Detection from Social Media
Authors: Yi Han, Shanika Karunasekera, Christopher Leckie
```
```
arXiv:2006.11343  
FakeCovid -- A Multilingual Cross-domain Fact Check News Dataset for COVID-19
Authors: Gautam Kishore Shahi, Durgesh Nandini
```
```
arXiv:2006.00885 
CoAID: COVID-19 Healthcare Misinformation Dataset
Authors: Limeng Cui, Dongwon Lee
```
```
arXiv:2005.13770  
DeepSonar: Towards Effective and Robust Detection of AI-Synthesized Fake Voices
Authors: Run Wang, Felix Juefei-Xu, Yihao Huang, Qing Guo, Xiaofei Xie, Lei Ma, Yang Liu
```
```
arXiv:2005.13270 
BRENDA: Browser Extension for Fake News Detection
Authors: Bjarte Botnevik, Eirik Sakariassen, Vinay Setty
```
```
arXiv:2005.04938  
A Deep Learning Approach for Automatic Detection of Fake News
Authors: Tanik Saikh, Arkadipta De, Asif Ekbal, Pushpak Bhattacharyya
```
```
arXiv:2004.12330  
Detecting fake news for the new coronavirus by reasoning on the Covid-19 ontology
Authors: Adrian Groza
```
```
arXiv:2004.11648 
GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media
Authors: Yi-Ju Lu, Cheng-Te Li
```
