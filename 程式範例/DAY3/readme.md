#
```



```

# seq2seq
```
Neural machine translation without attention
```
# attention
```
Neural machine translation with attention
https://tensorflow.google.cn/tutorials/text/nmt_with_attention
```

# Transformer model
```
Attention Is All You Need
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
https://arxiv.org/abs/1706.03762
```
```
Transformer model for language understanding
https://tensorflow.google.cn/tutorials/text/transformer
```
# Pre-Trained model
```
GPT 
```
# BERT(2018)
```
Devlin, J., Chang, M.-W., Lee, K., and Toutanova,K. (2019). 
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
arXiv:1810.04805 
```
```
Fine-tuning a BERT model
https://tensorflow.google.cn/official_models/fine_tuning_bert
```
### GPT-3
```
Language Models are Few-Shot Learners
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, 
Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, 
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, 
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei

https://arxiv.org/abs/2005.14165
https://github.com/openai/gpt-3
[research paper review] GPT-3 : Language Models are Few-Shot Learners
https://www.youtube.com/watch?v=Mq97CF02sRY

GPT-3: Language Models are Few-Shot Learners (Paper Explaine
https://www.youtube.com/watch?v=SY5PvZrJhLE
```
