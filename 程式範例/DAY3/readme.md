# 主題
```
Neural machine translation(NMT)
Question Answering 
```
# Topic 1:Neural machine translation(NMT)

### seq2seq(2014)
```
Sequence to Sequence Learning with Neural Networks
Ilya Sutskever, Oriol Vinyals, Quoc V. Le
https://arxiv.org/abs/1409.3215
```
```
Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio
https://arxiv.org/abs/1406.1078
```
```
Sequence-to-sequence Learning
https://www.youtube.com/watch?v=ZjfjPzXw6og
```
```
Neural machine translation without attention
```
### attention
```
Attention Is All You Need
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
https://arxiv.org/abs/1706.03762
```
```
Neural machine translation with attention
https://tensorflow.google.cn/tutorials/text/nmt_with_attention
```


### Transformer model(2017)
```
Attention Is All You Need
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
https://arxiv.org/abs/1706.03762
```
```
Transformer model for language understanding
https://tensorflow.google.cn/tutorials/text/transformer
```
```
Transformer
https://www.youtube.com/watch?v=ugWDIIOHtPA
```
```
淺談神經機器翻譯 & 用 Transformer 與 TensorFlow 2 英翻中
https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html
```
### 大型語言模型
### ELMO(Embeddings from Language Models)(2018)
```
Matthew Peters et al., "Deep Contextualized Word Representations," Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies I (2018): 2227-2237.

Deep contextualized word representations
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer
https://arxiv.org/abs/1802.05365
```
```
ELMo原理解析及简单上手使用
https://zhuanlan.zhihu.com/p/51679783
```
```
ELMO, BERT, GPT
https://www.youtube.com/watch?v=UYPa347-DdE
```
### Universal Language Model Fine-tuning(ULM-Fit)[2018]
```
Universal Language Model Fine-tuning for Text Classification
Jeremy Howard, Sebastian Ruder
https://arxiv.org/abs/1801.06146

J. Howard et al., “Universal Language Model Fine-tuning for Text Classification”, 
Proc. of the 56th Annual Meeting of the Association for Computational Linguistics, 2018: 328-339
```
### RoBERTa(2019)
```
RoBERTa: A Robustly Optimized BERT Pretraining Approach
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, 
Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov
https://arxiv.org/abs/1907.11692
```
### XLM(2019)
```
Cross-lingual Language Model Pretraining
Guillaume Lample, Alexis Conneau
https://arxiv.org/abs/1901.07291
```
### Megatron-LM(2019)
```
Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro
https://arxiv.org/abs/1909.08053

https://github.com/NVIDIA/Megatron-LM
```
### XLNet(2019)
```
XLNet: Generalized Autoregressive Pretraining for Language Understanding
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le
https://arxiv.org/abs/1906.08237
```
```
中文XLNet预训练模型
https://github.com/ymcui/Chinese-XLNet
```
### BERT(2018)
```
Devlin, J., Chang, M.-W., Lee, K., and Toutanova,K. (2019). 
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
arXiv:1810.04805 
```
```
Fine-tuning a BERT model
https://tensorflow.google.cn/official_models/fine_tuning_bert
```
```
進擊的 BERT：NLP 界的巨人之力與遷移學習
2019-07-10 (Wed) 48,073 views
https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html
```
```
http://mccormickml.com/tutorials/

Pytorch
BERT Application Examples:
Word Embeddings (post, notebook)
Sentence Classification (post, notebook)
Document Classification (video, notebook)
```
### BERT and its family
```
BERT and its family
http://speech.ee.ntu.edu.tw/~tlkagk/courses/DLHLP20/BERT%20train%20(v8).pdf
```
#### DistilBERT(2019)
```
DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf
https://arxiv.org/abs/1910.01108
```
```
Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT
https://medium.com/huggingface/distilbert-8cf3380435b5
```
```
Distilling from BERT论文笔记
https://zhuanlan.zhihu.com/p/73046023
```
### Multilingual BERT
```
How multilingual is Multilingual BERT?
Telmo Pires, Eva Schlinger, Dan Garrette
https://arxiv.org/abs/1906.01502
```
```
http://speech.ee.ntu.edu.tw/~tlkagk/courses/DLHLP20/Multi%20(v2).pdf
```
### GPT 1-2-3
```
直觀理解 GPT-2 語言模型並生成金庸武俠小說
https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html

讓 AI 寫點金庸：如何用 TensorFlow 2.0 及 TensorFlow.js 寫天龍八部
https://leemeng.tw/how-to-generate-interesting-text-with-tensorflow2-and-tensorflow-js.html
```
### GPT-3(2020)
```
Language Models are Few-Shot Learners
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, 
Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, 
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, 
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, 
Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei

https://arxiv.org/abs/2005.14165
https://github.com/openai/gpt-3
```
```
[research paper review] GPT-3 : Language Models are Few-Shot Learners
https://www.youtube.com/watch?v=Mq97CF02sRY
```
```
http://speech.ee.ntu.edu.tw/~tlkagk/courses/DLHLP20/GPT3%20(v6).pdf

GPT-3: Language Models are Few-Shot Learners (Paper Explaine
https://www.youtube.com/watch?v=SY5PvZrJhLE
```
### 百度的預訓練語言模型ERNIE[2019]
```
ERNIE 2.0: A Continual Pre-training Framework for Language Understanding
Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, Haifeng Wang
https://arxiv.org/abs/1907.12412

http://research.baidu.com/Blog/index-view?id=128

https://github.com/PaddlePaddle/ERNIE
```
```
自然語言處理標竿2019最後一波測試，百度打敗微軟、Google
百度的預訓練語言模型ERNIE，在GLUE平台2019年底最後一次16項包含中英文的自然語言處理測試中拿下第一，表現優於Google、微軟和卡內基美隆大學
文/林妍溱 | 2020-01-01發表  https://www.ithome.com.tw/news/135127
```
# Chatbot
```
Google發表目前最先進的開放領域聊天機器人Meena
Meena模型具有26億個參數，使用341 GB文本訓練，其回應的合理性和具體性，與人類相差不遠
文/李建興 | 2020-01-31發表
https://www.ithome.com.tw/news/135572
```
```
Google更新Dialogflow AI引擎，使用者可創建更強大的虛擬客服
Dialogflow新增Mega Agent，提供高達2萬個意圖，使用者能創建對話能力更強的代理
文/李建興 | 2020-02-20發表
https://www.ithome.com.tw/news/135925
```
# Question Answering 
```

```
```
http://speech.ee.ntu.edu.tw/~tlkagk/courses_DLHLP20.html

[DLHLP 2020] Deep Learning for Question Answering (1/2) (重新上傳)
https://www.youtube.com/watch?v=gRfTfXCe3LA

https://www.youtube.com/watch?v=gRfTfXCe3LA
https://www.youtube.com/watch?v=h_Lptoq8spQ
```
# Fake Object Detection and Generation
```
Defending Against Neural Fake News
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi
https://arxiv.org/abs/1905.12616
```
```
使用NLP檢測和對抗AI生成的假新聞
磐创 AI 2020-06-14
https://blog.csdn.net/fendouaini/java/article/details/106744426

GROVER—A State-of-the-Art Defense against Neural Fake News
https://grover.allenai.org/

https://rowanzellers.com/grover/

https://github.com/rowanz/grover
```
```
arXiv:2007.03316  
Graph Neural Networks with Continual Learning for Fake News Detection from Social Media
Authors: Yi Han, Shanika Karunasekera, Christopher Leckie
```
```
arXiv:2006.11343  
FakeCovid -- A Multilingual Cross-domain Fact Check News Dataset for COVID-19
Authors: Gautam Kishore Shahi, Durgesh Nandini
```
```
arXiv:2006.00885 
CoAID: COVID-19 Healthcare Misinformation Dataset
Authors: Limeng Cui, Dongwon Lee
```
```
arXiv:2005.13770  
DeepSonar: Towards Effective and Robust Detection of AI-Synthesized Fake Voices
Authors: Run Wang, Felix Juefei-Xu, Yihao Huang, Qing Guo, Xiaofei Xie, Lei Ma, Yang Liu
```
```
arXiv:2005.13270 
BRENDA: Browser Extension for Fake News Detection
Authors: Bjarte Botnevik, Eirik Sakariassen, Vinay Setty
```
```
arXiv:2005.04938  
A Deep Learning Approach for Automatic Detection of Fake News
Authors: Tanik Saikh, Arkadipta De, Asif Ekbal, Pushpak Bhattacharyya
```
```
arXiv:2004.12330  
Detecting fake news for the new coronavirus by reasoning on the Covid-19 ontology
Authors: Adrian Groza
```
```
arXiv:2004.11648 
GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media
Authors: Yi-Ju Lu, Cheng-Te Li
```
# Text Generation
```
Google開源可即時產生精確文字的AI模型LaserTagger
LaserTagger的預測速度，是常用文字處理方法Seq2seq的100倍，可用在許多需要即時回應的場景
文/李建興 | 2020-02-03發表
https://www.ithome.com.tw/news/135610
https://ai.googleblog.com/2020/01/encode-tag-and-realize-controllable-and.html

https://github.com/google-research/lasertagger
```
