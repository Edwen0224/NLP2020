{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Preliminaries\n\nFirst install a critical dependency for our code"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install keras==2.2.4 # critical dependency","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Write requirements to file, anytime you run it, in case you have to go back and recover dependencies.\n\nLatest known such requirements are hosted for each notebook in the companion github repo, and can be pulled down and installed here if needed. Companion github repo is located at https://github.com/azunre/transfer-learning-for-nlp"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip freeze > kaggle_image_requirements.txt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import neural network libraries\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom keras import backend as K\nimport keras.layers as layers\nfrom keras.models import Model, load_model\nfrom keras.engine import Layer\n\n# Initialize tensorflow/keras session\nsess = tf.Session()\nK.set_session(sess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some other key imports\nimport os\nimport re\nimport pandas as pd\nimport numpy as np\nimport random","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Tokenization, Stop-word and Punctuation Removal Functions\n\nBefore proceeding, we must decide how many samples to draw from each class. We must also decide the maximum number of tokens per email, and the maximum length of each token. This is done by setting the following overarching hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"Nsamp = 1000 # number of samples to generate in each class - 'spam', 'not spam'\nmaxtokens = 50 # the maximum number of tokens per document\nmaxtokenlen = 20 # the maximum length of each token","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tokenization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(row):\n    if row is None or row is '':\n        tokens = \"\"\n    else:\n        tokens = row.split(\" \")[:maxtokens]\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Use regular expressions to remove unnecessary characters**\n\nNext, we define a function to remove punctuation marks and other nonword characters (using regular expressions) from the emails with the help of the ubiquitous python regex library. In the same step, we truncate all tokens to hyperparameter maxtokenlen defined above."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndef reg_expressions(row):\n    tokens = []\n    try:\n        for token in row:\n            token = token.lower()\n            token = re.sub(r'[\\W\\d]', \"\", token)\n            token = token[:maxtokenlen] # truncate token\n            tokens.append(token)\n    except:\n        token = \"\"\n        tokens.append(token)\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Stop-word removal**\n\nLet’s define a function to remove stopwords - words that occur so frequently in language that they offer no useful information for classification. This includes words such as “the” and “are”, and the popular library NLTK provides a heavily-used list that will employ."},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words('english')    \n\n# print(stopwords) # see default stopwords\n# it may be beneficial to drop negation words from the removal list, as they can change the positive/negative meaning\n# of a sentence - but we didn't find it to make a difference for this problem\n# stopwords.remove(\"no\")\n# stopwords.remove(\"nor\")\n# stopwords.remove(\"not\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stop_word_removal(row):\n    token = [token for token in row if token not in stopwords]\n    token = filter(None, token)\n    return token","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Download and Assemble IMDB Review Dataset\n\nDownload the labeled IMDB reviews"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget -q \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n!tar xzf aclImdb_v1.tar.gz","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shuffle and preprocess data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function for shuffling data\ndef unison_shuffle(data, header):\n    p = np.random.permutation(len(header))\n    data = data[p]\n    header = np.asarray(header)[p]\n    return data, header\n\ndef load_data(path):\n    data, sentiments = [], []\n    for folder, sentiment in (('neg', 0), ('pos', 1)):\n        folder = os.path.join(path, folder)\n        for name in os.listdir(folder):\n            with open(os.path.join(folder, name), 'r') as reader:\n                  text = reader.read()\n            text = tokenize(text)\n            text = stop_word_removal(text)\n            text = reg_expressions(text)\n            data.append(text)\n            sentiments.append(sentiment)\n    data_np = np.array(data)\n    data, sentiments = unison_shuffle(data_np, sentiments)\n    \n    return data, sentiments\n\ntrain_path = os.path.join('aclImdb', 'train')\ntest_path = os.path.join('aclImdb', 'test')\nraw_data, raw_header = load_data(train_path)\n\nprint(raw_data.shape)\nprint(len(raw_header))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Subsample required number of samples\nrandom_indices = np.random.choice(range(len(raw_header)),size=(Nsamp*2,),replace=False)\ndata_train = raw_data[random_indices]\nheader = raw_header[random_indices]\n\nprint(\"DEBUG::data_train::\")\nprint(data_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Display sentiments and their frequencies in the dataset, to ensure it is roughly balanced between classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_elements, counts_elements = np.unique(header, return_counts=True)\nprint(\"Sentiments and their frequencies:\")\nprint(unique_elements)\nprint(counts_elements)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function for converting data into the right format, due to the difference in required format from sklearn models\n# we expect a single string per email here, versus a list of tokens for the sklearn models previously explored\ndef convert_data(raw_data,header):\n    converted_data, labels = [], []\n    for i in range(raw_data.shape[0]):\n        # combine list of tokens representing each email into single string\n        out = ' '.join(raw_data[i])\n        converted_data.append(out)\n        labels.append(header[i])\n    converted_data = np.array(converted_data, dtype=object)[:, np.newaxis]\n    \n    return converted_data, np.array(labels)\n\ndata_train, header = unison_shuffle(data_train, header)\n\n# split into independent 70% training and 30% testing sets\nidx = int(0.7*data_train.shape[0])\n# 70% of data for training\ntrain_x, train_y = convert_data(data_train[:idx],header[:idx])\n# remaining 30% for testing\ntest_x, test_y = convert_data(data_train[idx:],header[idx:])\n\nprint(\"train_x/train_y list details, to make sure it is of the right form:\")\nprint(len(train_x))\nprint(train_x)\nprint(train_y[:5])\nprint(train_y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build, Train and Evaluate ELMo Model\nCreate a custom tf hub ELMO embedding layer"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class ElmoEmbeddingLayer(Layer):\n    def __init__(self, **kwargs):\n        self.dimensions = 1024 # initialize output dimension of ELMo embedding\n        self.trainable=True\n        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape): # function for building ELMo embedding\n        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable,\n                               name=\"{}_module\".format(self.name)) # download pretrained ELMo model\n        # extract trainable parameters, which are only a small subset of the total - this is a constraint of\n        # the tf hub module as shared by the authors - see https://tfhub.dev/google/elmo/2\n        # the trainable parameters are 4 scalar weights on the sum of the outputs of ELMo layers\n        \n        self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n        super(ElmoEmbeddingLayer, self).build(input_shape)\n\n    def call(self, x, mask=None): # specify function for calling embedding\n        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n                      as_dict=True,\n                      signature='default',\n                      )['default']\n        return result\n\n    def compute_output_shape(self, input_shape): # specify output shape\n        return (input_shape[0], self.dimensions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now use the custom TF hub ELMo embedding layer within a higher-level function to define the overall model. More specifically, we put a dense trainable layer of output dimension 256 on top of the ELMo embedding."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to build model\ndef build_model(): \n    input_text = layers.Input(shape=(1,), dtype=\"string\")\n    embedding = ElmoEmbeddingLayer()(input_text)\n    dense = layers.Dense(256, activation='relu')(embedding)\n    pred = layers.Dense(1, activation='sigmoid')(dense)\n    model = Model(inputs=[input_text], outputs=pred)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.summary()\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build and fit\nmodel = build_model()\nhistory = model.fit(train_x, \n          train_y,\n          validation_data=(test_x, test_y),\n          epochs=5,\n          batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Save trained model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('ElmoModel.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualize Convergence**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndf_history = pd.DataFrame(history.history)\n\nfig,ax = plt.subplots()\nplt.plot(range(df_history.shape[0]),df_history['val_acc'],'bs--',label='validation')\nplt.plot(range(df_history.shape[0]),df_history['acc'],'r^--',label='training')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.title('ELMo Movie Review Classification Training')\nplt.legend(loc='best')\nplt.grid()\nplt.show()\n\nfig.savefig('ELMoConvergence.eps', format='eps')\nfig.savefig('ELMoConvergence.pdf', format='pdf')\nfig.savefig('ELMoConvergence.png', format='png')\nfig.savefig('ELMoConvergence.svg', format='svg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Make figures downloadable to local system in interactive mode**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\ndef create_download_link(title = \"Download file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(filename='ELMoConvergence.svg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# you must remove all downloaded files - having too many of them on completion will make Kaggle reject your notebook \n!rm -rf aclImdb\n!rm aclImdb_v1.tar.gz","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}